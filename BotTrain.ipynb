{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a359d4cd-9181-4429-b879-ef89c4d083c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a1ad4-4e1d-4238-93fe-de6ca2c0e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data (wordnet, punkt)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f13b7-ed83-485f-9f8d-4ae18cba4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f858fe25-c9cf-48da-9a88-a3c0c52e6531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the intents file\n",
    "with open('intents.json') as file:\n",
    "    intents = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6429bde-1211-4528-8dd9-8af6bd83892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0409c0-eec0-4e23-8b12-ac09b2a1772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the intents\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        # Add to documents\n",
    "        documents.append((w, intent['tag']))\n",
    "        # Add to classes if not already present\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f7cefd-d1b5-4fb8-ac87-a1029cffb678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize and lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec83f7d-f7e9-48cd-8c45-606f7a54a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort classes\n",
    "classes = sorted(list(set(classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0221ac-6de2-4f63-8a49-900da5fb6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(documents)} documents\")\n",
    "print(f\"{len(classes)} classes: {classes}\")\n",
    "print(f\"{len(words)} unique lemmatized words: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d44445-31b0-4290-a760-8752bed68189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save words and classes to pickle files\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702a6d6-896e-48c8-99b5-072909c3b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "training = []\n",
    "output_empty = [0] * len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a26afa-6679-43ce-b4e0-8676fcb11d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    # Initialize bag of words\n",
    "    bag = []\n",
    "    # Tokenize the pattern words\n",
    "    pattern_words = doc[0]\n",
    "    # Lemmatize each word\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # Create bag of words\n",
    "    bag = [1 if w in pattern_words else 0 for w in words]\n",
    "\n",
    "    # Output is '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205d308-7d76-4636-8c1c-a91424ef85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle training data and convert to numpy array\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "train_x = np.array(list(training[:, 0]))\n",
    "train_y = np.array(list(training[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077f6a3-c267-4a9f-b93f-4289d3d7b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure the training data is craeted\n",
    "print(\"Training data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31d1b5-a249-469f-a92d-58e720896164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed376616-a75a-48a4-b293-40eba1cebde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b2ba5-cd83-4e70-86b5-d5e405086d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "hist = model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b8300c-7ec9-46d3-a1f0-55725e4e433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('chatbot_model.h5', hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b528806b-04e9-4249-b17b-b5d6701a8159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure the modelis created\n",
    "print(\"Model created and saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
